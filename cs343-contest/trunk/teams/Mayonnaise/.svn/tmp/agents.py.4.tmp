# agents.py
# -----------------
# Licensing Information: Please do not distribute or publish solutions to this
# project. You are free to use and extend these projects for educational
# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by
# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).
# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html

from captureAgents import CaptureAgent
from captureAgents import AgentFactory
import distanceCalculator
import random, time, util, math
from game import Directions, Actions
import keyboardAgents
import game
from util import nearestPoint
from pprint import pprint

# global variables
BELIEFS = util.Counter() # beliefs of position of enemies - all the emenies share the same counter
INVADED = False
MAXDEPTH = 1 # for minimax tree

#############
# FACTORIES #
#############

NUM_KEYBOARD_AGENTS = 0
class Mayonnaise(AgentFactory):
  "Returns one keyboard agent and offensive reflex agents"

  def __init__(self, isRed, first='offense', second='defense', rest='offense'):
    AgentFactory.__init__(self, isRed)
    self.agents = [first, second]
    self.rest = rest

  def getAgent(self, index):
    if len(self.agents) > 0:
      return self.choose(self.agents.pop(0), index)
    else:
      return self.choose(self.rest, index)

  def choose(self, agentStr, index):
    if agentStr == 'keys':
      global NUM_KEYBOARD_AGENTS
      NUM_KEYBOARD_AGENTS += 1
      if NUM_KEYBOARD_AGENTS == 1:
        return keyboardAgents.KeyboardAgent(index)
      elif NUM_KEYBOARD_AGENTS == 2:
        return keyboardAgents.KeyboardAgent2(index)
      else:
        raise Exception('Max of two keyboard agents supported')
    elif agentStr == 'offense':
      return OffensiveReflexAgent(index)
    elif agentStr == 'defense':
      return DefensiveReflexAgent(index)
    else:
      raise Exception("No staff agent identified by " + agentStr)


class AllOffenseAgents(AgentFactory):
  "Returns one keyboard agent and offensive reflex agents"

  def __init__(self, **args):
    AgentFactory.__init__(self, **args)

  def getAgent(self, index):
    return OffensiveReflexAgent(index)

class OffenseDefenseAgents(AgentFactory):
  "Returns one keyboard agent and offensive reflex agents"

  def __init__(self, **args):
    AgentFactory.__init__(self, **args)
    self.offense = False

  def getAgent(self, index):
    self.offense = not self.offense
    if self.offense:
      return OffensiveReflexAgent(index)
    else:
      return DefensiveReflexAgent(index)

##########
# Agents #
##########

class ReflexCaptureAgent(CaptureAgent):
  """
  A base class for reflex agents that chooses score-maximizing actions
  """

  def chooseAction(self, gameState, depth = 0):
    """
    Picks among the actions with the highest Q(s,a).
    """
    global BELIEFS
    beliefs = BELIEFS

    start = time.time()
    action = self.chooseActionPair(gameState, beliefs, 0)[0]
    print 'eval time for agent %d: %.4f' % (self.index, time.time() - start)

    return action

  def chooseActionPair(self, gameState, beliefs, depth):
    """
    sometimes we need both (action, value), that's why this method
    """
    actions = gameState.getLegalActions(self.index)

<<<<<<< .mine
    # this is about DBN. We'll update the beliefs according to our combined efforts 
    self.elapseTime(gameState)
=======
>>>>>>> .r40
    # all agents on our side are responsible for update beliefs according to what they see
    # definitely, for REAL world
    if depth == 0:
      # update real beliefs
      self.observe(gameState, beliefs)

    # go one step further
    self.elapseTime(gameState, beliefs)

    # You can profile your evaluation time by uncommenting these lines
<<<<<<< .mine
    # start = time.time()
    values = [self.evaluate(gameState, a) for a in actions]
    # TODO test
    print self.index, actions, values
    # print 'eval time for agent %d: %.4f' % (self.index, time.time() - start)
=======
    values = [self.evaluate(gameState, a, beliefs, depth) for a in actions]
>>>>>>> .r40

    maxValue = max(values)
    bestActions = [a for a, v in zip(actions, values) if v == maxValue]

    # return (action, value) pair
    return (random.choice(bestActions), maxValue)

  def registerInitialState(self, gameState):
    """
    Initialize the beliefs of the enemies uniformly
    and many related operations
    """
    CaptureAgent.registerInitialState(self, gameState)

    # legalPositions is a useful variable. it's not provided! let's construct it
    width = gameState.data.layout.width
    height = gameState.data.layout.height
    self.legalPositions = [(x, y) for x in range(width) for y in range(height) if not gameState.hasWall(x, y)]

    # set legalPositions for defending and offending respectively
    self.defensiveLegalPositions = [p for p in self.legalPositions if self.isOurTerrain(gameState, p)]
    self.offensiveLegalPositions = [p for p in self.legalPositions if not (p in self.defensiveLegalPositions)]

    # initialize beliefs according to legalPositions
    for enemyId in self.getOpponents(gameState):
      self.initializeBeliefs(enemyId)

    # set availableActions for each grid
    self.setNeighbors(gameState)

    # set boundaries - entries of the enemy!
    self.setBoundary(gameState)

    # set initial position
    self.initialPosition = gameState.getAgentState(self.index).getPosition()

    # set trainsition model
    self.transition = util.Counter()
    self.transition['favor'] = 0.8
    self.transition['indifferent'] = 0.05

  def initializeBeliefs(self, enemyId):
    global BELIEFS
    beliefs = BELIEFS
    beliefs[enemyId] = util.Counter() 
    for pos in self.offensiveLegalPositions:
      beliefs[enemyId][pos] = 1
      beliefs[enemyId].normalize()
  
  def observe(self, gameState, beliefs):
    """
    for DBN. change the beliefs after observation
    this should be called by each agent on our side
    """
    myPos = gameState.getAgentState(self.index).getPosition()
    allNoiseDistances = gameState.getAgentDistances() # this is for all; obviously, we don't need our own "noisy" positions

    # 1st way: SONAR
    for i in self.getOpponents(gameState):
      noiseDistance = allNoiseDistances[i]
      for p in self.legalPositions:
	trueDistance = util.manhattanDistance(p, myPos)
	if trueDistance < 5:
	  # this kind of observation can be got in 2nd way
	  beliefs[i][p] = 0
	else:
	  beliefs[i][p] *= gameState.getDistanceProb(trueDistance, noiseDistance)

    # 2nd way: whether we can directly see it!
    for i in self.getOpponents(gameState):
      enemy = gameState.getAgentState(i)
      pos = enemy.getPosition()
      if pos != None:
        # there's REALLY an enemy here
        beliefs[i][pos] = 1

    # 3rd way: infered from food
    foodNow = self.getFoodYouAreDefending(gameState).asList()
    for pos in foodNow:
      for i in self.getOpponents(gameState):
        beliefs[i][pos] = 0 # well, food is there - no enemy there

    previousState = self.getPreviousObservation()
    if previousState != None:
      # when this is not the initial state
      foodPrevious = self.getFoodYouAreDefending(previousState).asList()
      difference = [p for p in self.legalPositions if (p in foodNow) != (p in foodPrevious)]
      # missing dot on our terrain?! I SEE YOU, invader!
      for pos in difference:
        # we need to check which enemy eat that dot
	# we use expectation of the enemy position, attribute this to the nearest enemy
	trueAgent = min([i for i in self.getOpponents(gameState)], key=lambda i: self.getMazeDistance(pos, self.mostLikelyPosition(gameState, self.defensiveLegalPositions, i)[0]))

    for i in self.getOpponents(gameState):
      beliefs[i].normalize()

      # in case some of enemy is missed
      if sum([value for value in beliefs[i].values()]) == 0:
        # redistribute
        self.initializeBeliefs(i)

  def elapseTime(self, gameState, beliefs):
    """
    for DBN. change the beliefs after one round of actions for all agent
    this should be called by only the first agent on our side
    """
    newBeliefs = util.Counter() # P(G_t|N_1..t-1)
    
    # now, according to our transition model, redistribute these probabilities
<<<<<<< .mine
    i = self.index - 1
    if i == -1: i = gameState.getNumAgents() - 1

    newBelief = util.Counter()
    for nowPosition in self.legalPositions:
      if self.isOurTerrain(gameState, nowPosition):
	# this pacman will choose the neighbor which closest to a food
	findMin = self.findNearestPathToFood
      else:
	# this ghost will choose the neighbor which closest to our pacman
	findMin = self.findNearestPathToPacman

      myPos = gameState.getAgentPosition(self.index)
      minNeighbor = findMin(gameState, self.neighbors[nowPosition])
      # for each neighbor of nowPosition, find its probability after transition
      for neighbor in self.neighbors[nowPosition]:
	if neighbor != minNeighbor:
	  # it's just a common neighbor
	  newBelief[neighbor] += self.transition['indifferent'] * beliefs[i][nowPosition]
=======
    # i is the id of agent we want to update
    i = self.index - 1
    newBeliefs[i] = util.Counter()
    for nowPosition in self.legalPositions:
      if self.isOurTerrain(gameState, nowPosition):
	# this pacman will choose the neighbor which closest to a food
	findMin = self.findNearestPathToFood
      else:
	# this ghost will choose the neighbor which closest to our pacman
	findMin = self.findNearestPathToPacman

      myPos = gameState.getAgentPosition(self.index)
      minNeighbor = findMin(gameState, self.neighbors[nowPosition])
      # for each neighbor of nowPosition, find its probability after transition
      for neighbor in self.neighbors[nowPosition]:
	if neighbor != minNeighbor:
	  # it's just a common neighbor
	  newBeliefs[i][neighbor] += self.transition['indifferent'] * beliefs[i][nowPosition]
>>>>>>> .r40
	else:
<<<<<<< .mine
	  # for most likely direction, put more weight on it
	  newBelief[neighbor] += self.transition['favor'] * beliefs[i][nowPosition]
=======
	  # for most likely direction, put more weight on it
	  newBeliefs[i][neighbor] += self.transition['favor'] * beliefs[i][nowPosition]
>>>>>>> .r40

<<<<<<< .mine
    newBelief.normalize()
=======
    newBeliefs[i].normalize()
>>>>>>> .r40

    # update the beliefs with new time slice
<<<<<<< .mine
    beliefs[i] = newBelief
=======
    beliefs[i] = newBeliefs[i]
>>>>>>> .r40

    # this is used for drawing distribution
    self.displayDistributionsOverPositions(gameState, beliefs)

  def displayDistributionsOverPositions(self, gameState, beliefs):
    dists = []
    for i in range(gameState.getNumAgents()):
      if i in beliefs.keys():
        dists.append(beliefs[i])
      else:
        dists.append(util.Counter())
    self.display.updateDistributions(dists)

  def findNearestPathToPacman(self, gameState, positions):
    pacmanPos = gameState.getAgentPosition(self.index)
    distanceFunc = lambda position : self.getMazeDistance(pacmanPos, position)
    return min(positions, distanceFunc)

  def findNearestPathToFood(self, gameState, positions):
    foodList = self.getFoodYouAreDefending(gameState).asList() + self.getCapsules(gameState)
    minDis = 99999
    minPos = None
    for pos in positions:
      for food in foodList:
        dis = self.getMazeDistance(pos, food)
	if dis < minDis:
	  minDis = dis
	  minPos = pos

    return minPos

  def getSuccessor(self, gameState, action):
    """
    Finds the next successor which is a grid position (location tuple).
    """
    successor = gameState.generateSuccessor(self.index, action)
    pos = successor.getAgentState(self.index).getPosition()
    if pos != nearestPoint(pos):
      # Only half a grid position was covered
      return successor.generateSuccessor(self.index, action)
    else:
      return successor

<<<<<<< .mine
  def evaluate(self, gameState, action):
    """
    Computes a linear combination of features and feature weights
    """
    features = self.getFeatures(gameState, action)
    weights = self.getWeights(gameState, action)
    print action, features
    return features * weights

=======
>>>>>>> .r40
  def getFeatures(self, gameState, action):
    """
    Returns a counter of features for the state
    """
    features = util.Counter()
    successor = self.getSuccessor(gameState, action)
    features['successorScore'] = self.getScore(successor)
    return features

  def getWeights(self, gameState, action):
    """
    Normally, weights do not depend on the gamestate.  They can be either
    a counter or a dictionary.
    """
    return {'successorScore': 1.0}

  def isOurTerrain(self, gameState, pos):
    """
    Check if pos is in our terrain
    """
    if self.red:
      return gameState.isRed(pos)
    else:
      return not gameState.isRed(pos)

  def setNeighbors(self, gameState):
    """
    this will set self.availableActions
    this is different from legalActions, which is for agents. this is for each grid and, thus, static
    """
    self.neighbors = util.Counter()
    for pos in self.legalPositions:
      self.neighbors[pos] = Actions.getLegalNeighbors(pos, gameState.data.layout.walls)

  def setBoundary(self, gameState):
    """
    set boundary in self.boudary, which is positions in our terrain which near the enemy side
    """
    self.boundary = []
    for pos in self.offensiveLegalPositions:
      for neighbor in self.neighbors[pos]:
        if self.isOurTerrain(gameState, neighbor):
	  self.boundary.append(neighbor)

  def mostLikelyPosition(self, gameState, positions, i = None):
    """
    find the max belief position in positions, i set the agent number
    if i == None, it finds global most likely position among all the agents
    return (position, prob)
    """
    beliefs = BELIEFS
    maxPosition = None
    belief = -1 # make sure it can be replaced by someone
    if i != None:
      for pos in positions:
        if beliefs[i][pos] > belief:
	  belief = beliefs[i][pos]
	  maxPosition = pos
      return (maxPosition, belief)
    else:
      for i in self.getOpponents(gameState):
        currentPos = self.mostLikelyPosition(gameState, positions, i)
	if currentPos[1] > belief:
	  maxPosition = currentPos[0]
	  belief = currentPos[1]
      return (maxPosition, belief)

class OffensiveReflexAgent(ReflexCaptureAgent):
  """
  A reflex agent that seeks food. This is an agent
  we give you to get an idea of what an offensive agent might look like,
  but it is by no means the best or only way to build an offensive agent.
  """

  def registerInitialState(self, gameState):
    """
    Initialize the beliefs of the enemies uniformly
    and many related operations
    """
    ReflexCaptureAgent.registerInitialState(self, gameState)

    self.defending = False # initialy, it's not defending
    self.maxDepth = MAXDEPTH

<<<<<<< .mine
  def getFeatures(self, gameState, action):
    if invaded and self.isOurTerrain(gameState, gameState.getAgentState(self.index).getPosition()):
      self.defending = True
=======
  def evaluate(self, gameState, action, beliefs, depth):
    """
    Computes a linear combination of features and feature weights
    """
    nextBeliefs = beliefs.copy()
    successor = self.getSuccessor(gameState, action)
>>>>>>> .r40
<<<<<<< .mine
    else:
=======

    if depth < self.maxDepth:
      # again. it's like chooseAction - evaluate - chooseAction ...
      decision = self.chooseActionPair(successor, nextBeliefs, depth + 1)
      return decision[1]
    else:
      features = self.getFeatures(successor, nextBeliefs)
      weights = self.getWeights(successor)
      return features * weights

  def getFeatures(self, successor, beliefs):
    if INVADED and self.isOurTerrain(successor, successor.getAgentState(self.index).getPosition()):
      self.defending = True
    else:
>>>>>>> .r40
      self.defending = False
    
    if self.defending:
      # defending!
      return DefensiveReflexAgent.staticGetFeatures(self, successor, beliefs)
    else:
      # offending!
      features = util.Counter()
      enemies = [successor.getAgentState(i) for i in self.getOpponents(successor)]
      ghosts = [a for a in enemies if not a.isPacman and a.getPosition() != None]
      features['successorScore'] = self.getScore(successor)
      foodList = self.getFood(successor).asList() + self.getCapsules(successor)
      myPos = successor.getAgentState(self.index).getPosition()
    
      # if capsule
      if myPos in self.getCapsules(gameState):
        print "Pick!"
        features['capsule'] = 1
 
      # distance to the most likely ghost position
      if self.isOurTerrain(successor, myPos):
	features['danger'] = 0
      else:
	ghostPosition = self.mostLikelyPosition(successor, self.offensiveLegalPositions)[0]

	# maybe our observation is scared. In that case, noWorry == True
	# I tried to implement this in belief, but failed - defensive agent does care about where the ghost is.
	# It would become pacman immediately after invading even scared
	numOfScared = 0
	for ghost in ghosts:
	  if ghost.scaredTimer > 0:
	    numOfScared += 1 # okay, what we find is just a scared ghost

	likelyDistance = self.getMazeDistance(myPos, ghostPosition)
	if numOfScared == 0 and likelyDistance < 3:
	  features['danger'] = 3 - likelyDistance
	else:
	  features['danger'] = 0
<<<<<<< .mine

	features['numOfScared'] = numOfScared

      # Compute distance to the nearest food
      if len(foodList) > 0: # This should always be True,  but better safe than sorry
        # if not invaded:
          # we'd rather not die now - find pickable food
	# else:
	# otherwise, just pick any
	minDistance = min([self.getMazeDistance(myPos, food) for food in foodList])
	features['distanceToFood'] = minDistance

=======
	
>>>>>>> .r40
      return features

  def getWeights(self, gameState):
    if self.defending:
      # defend!
      return DefensiveReflexAgent.staticGetWeights(self, gameState)
    else:
      # offend!
<<<<<<< .mine
      return {'successorScore': 100, 'capsule': 100, 'distanceToFood': -1, 'danger': -50, 'numOfScared': -2, 'depth': -1000}
=======
      return {'successorScore': 100, 'distanceToFood': -1, 'danger': -200}
>>>>>>> .r40

class DefensiveReflexAgent(ReflexCaptureAgent):
  """
  A reflex agent that keeps its side Pacman-free. Again,
  this is to give you an idea of what a defensive agent
  could be like.  It is not the best or only way to make
  such an agent.
  """
  def evaluate(self, gameState, action, beliefs, depth):
    """
    Computes a linear combination of features and feature weights
    """
    nextBeliefs = beliefs.copy()
    successor = self.getSuccessor(gameState, action)

    features = self.getFeatures(successor, nextBeliefs)
    weights = self.getWeights(successor)
    return features * weights

  def getFeatures(self, successor, beliefs):
    global INVADED

    features = util.Counter()

    myState = successor.getAgentState(self.index)
    myPos = myState.getPosition()

    # Computes whether we're on defense (1) or offense (0), encourage it to defense
    features['onDefense'] = 1
    if myState.isPacman: features['onDefense'] = 0

    # here are tupes with (position, prob)
    pacmanPos = self.mostLikelyPosition(successor, self.defensiveLegalPositions)
    ghostPos = self.mostLikelyPosition(successor, self.offensiveLegalPositions)
    if pacmanPos[1] != 0:
      # sensitive to pacman - any clue is regarded as invaded
      aimPosition = pacmanPos[0]
      # here, if the agent is invaded, send this signal to global context
      # offender, knowing this, might eat dangerous(deep in maze) dots, thus got killed, and came back for help!
      INVADED = True
    else:
      aimPosition = ghostPos[0]
      INVADED = False
    likelyDistance = self.getMazeDistance(myPos, aimPosition)
    features['invaderDistance'] = likelyDistance

    return features

  staticGetFeatures = staticmethod(getFeatures)

  def getWeights(self, gameState):
    return {'onDefense': 10000, 'invaderDistance': -1}
  staticGetWeights = staticmethod(getWeights)
