# agents.py
# -----------------
# Licensing Information: Please do not distribute or publish solutions to this
# project. You are free to use and extend these projects for educational
# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by
# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).
# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html

from captureAgents import CaptureAgent
from captureAgents import AgentFactory
import distanceCalculator
import random, time, util
from game import Directions, Actions
import keyboardAgents
import game
from util import nearestPoint
from pprint import pprint

# global variables
beliefs = util.Counter() # beliefs of position of enemies - all the emenies share the same counter

#############
# FACTORIES #
#############

NUM_KEYBOARD_AGENTS = 0
class Mayonnaise(AgentFactory):
  "Returns one keyboard agent and offensive reflex agents"

  def __init__(self, isRed, first='offense', second='defense', rest='offense'):
    AgentFactory.__init__(self, isRed)
    self.agents = [first, second]
    self.rest = rest

  def getAgent(self, index):
    if len(self.agents) > 0:
      return self.choose(self.agents.pop(0), index)
    else:
      return self.choose(self.rest, index)

  def choose(self, agentStr, index):
    if agentStr == 'keys':
      global NUM_KEYBOARD_AGENTS
      NUM_KEYBOARD_AGENTS += 1
      if NUM_KEYBOARD_AGENTS == 1:
        return keyboardAgents.KeyboardAgent(index)
      elif NUM_KEYBOARD_AGENTS == 2:
        return keyboardAgents.KeyboardAgent2(index)
      else:
        raise Exception('Max of two keyboard agents supported')
    elif agentStr == 'offense':
      return OffensiveReflexAgent(index)
    elif agentStr == 'defense':
      return DefensiveReflexAgent(index)
    else:
      raise Exception("No staff agent identified by " + agentStr)


class AllOffenseAgents(AgentFactory):
  "Returns one keyboard agent and offensive reflex agents"

  def __init__(self, **args):
    AgentFactory.__init__(self, **args)

  def getAgent(self, index):
    return OffensiveReflexAgent(index)

class OffenseDefenseAgents(AgentFactory):
  "Returns one keyboard agent and offensive reflex agents"

  def __init__(self, **args):
    AgentFactory.__init__(self, **args)
    self.offense = False

  def getAgent(self, index):
    self.offense = not self.offense
    if self.offense:
      return OffensiveReflexAgent(index)
    else:
      return DefensiveReflexAgent(index)

##########
# Agents #
##########

class ReflexCaptureAgent(CaptureAgent):
  """
  A base class for reflex agents that chooses score-maximizing actions
  """

  def chooseAction(self, gameState):
    """
    Picks among the actions with the highest Q(s,a).
    """
    actions = gameState.getLegalActions(self.index)

    # this is about DBN. We'll update the beliefs according to our combined efforts 
    if self.index == self.getTeam(gameState)[0]:
      # the first agent on our side is responsible for updating time
      # Each agent will take only one step, definitely
      self.elapseTime(gameState)
    # all agents on our side are responsible for update beliefs according to what they see
    self.observe(gameState)

    # You can profile your evaluation time by uncommenting these lines
    # start = time.time()
    values = [self.evaluate(gameState, a) for a in actions]
    # print 'eval time for agent %d: %.4f' % (self.index, time.time() - start)

    maxValue = max(values)
    bestActions = [a for a, v in zip(actions, values) if v == maxValue]

    return random.choice(bestActions)

  def registerInitialState(self, gameState):
    """
    Initialize the beliefs of the enemies uniformly
    """
    CaptureAgent.registerInitialState(self, gameState)

    global beliefs

    # legalPositions is a useful variable. it's not provided! let's construct it
    width = gameState.data.layout.width
    height = gameState.data.layout.height
    self.legalPositions = [(x, y) for x in range(width) for y in range(height) if not gameState.hasWall(x, y)]

    # set legalPositions for defending and offending respectively
    self.defensiveLegalPositions = [p for p in self.legalPositions if self.isOurTerrain(gameState, p)]
    self.offensiveLegalPositions = [p for p in self.legalPositions if not (p in self.defensiveLegalPositions)]
    # also consider offensive positions which are too near our terrain
    moniteringPositions = [p for p in self.offensiveLegalPositions for q in self.defensiveLegalPositions if self.getMazeDistance(p, q) < 2]
    self.defensiveLegalPositions += moniteringPositions

    # initialize beliefs according to legalPositions
    for pos in self.offensiveLegalPositions:
      beliefs[pos] = 1

    beliefs.normalize()

    # set availableActions for each grid
    self.setNeighbors(gameState)

    # set trainsition model
    self.transition = util.Counter()
    self.transition['favor'] = 0.8
    self.transition['indifferent'] = 0.05

  def observe(self, gameState):
    """
    for DBN. change the beliefs after observation
    this should be called by each agent on our side
    """
    global beliefs
    myPos = gameState.getAgentState(self.index).getPosition()
    allNoiseDistances = gameState.getAgentDistances() # this is for all; obviously, we don't need our own "noisy" positions
    noiseDistances = [allNoiseDistances[i] for i in self.getOpponents(gameState)]

    # 1st way: SONAR
    for p in self.legalPositions:
      trueDistance = util.manhattanDistance(p, myPos)
      # again, for enemies, we don't care who is who - beliefs are just of the probability that there's an enemy there
      # otherwise it's too complicated to distinguish who sends the signals
      if trueDistance < 5:
        # this kind of observation can be got in 2nd way
        beliefs[p] = 0
      else:
        beliefs[p] *= sum([gameState.getDistanceProb(trueDistance, noiseDistance) for noiseDistance in noiseDistances])

    # 2nd way: whether we can directly see it!
    enemies = [gameState.getAgentState(i) for i in self.getOpponents(gameState)]
    for enemy in enemies:
      pos = enemy.getPosition()
      if pos != None:
        # there's REALLY an enemy here
  	# after normalizing, it becomes the highest probablity
        beliefs[pos] = 10

    # 3rd way: infered from food
    foodNow = self.getFoodYouAreDefending(gameState).asList()
    for pos in foodNow:
      beliefs[pos] = 0 # well, food is there - no enemy there
    previousState = self.getPreviousObservation()
    if previousState != None:
      # when this is not the initial state
      foodPrevious = self.getFoodYouAreDefending(previousState).asList()
      difference = [p for p in self.legalPositions if (p in foodNow) != (p in foodPrevious)]
      # missing dot on our terrain?! I SEE YOU, invader!
      for pos in difference:
        # maybe it moves to some place around. In that case, we can see it in 2nd way
	# this way just help us to rush to that invader
        beliefs[pos] = 8

    beliefs.normalize()

  def elapseTime(self, gameState):
    """
    for DBN. change the beliefs after one round of actions for all agent
    this should be called by only the first agent on our side
    """
    global beliefs
    newBeliefs = util.Counter() # P(G_t|N_1..t-1)
    
    # now, according to our transition model, redistribute these probabilities
    for nowPosition in self.legalPositions:
      if self.isOurTerrain(gameState, nowPosition):
        # this pacman will choose the neighbor which closest to a food
        findMin = self.findNearestPathToFood
      else:
        # this ghost will choose the neighbor which closest to our pacman
        findMin = self.findNearestPathToPacman

      myPos = gameState.getAgentPosition(self.index)
      minNeighbor = findMin(gameState, self.neighbors[nowPosition])
      # for each neighbor of nowPosition, find its probability after transition
      for neighbor in self.neighbors[nowPosition]:
        if neighbor != minNeighbor:
	  # it's just a common neighbor
  	  newBeliefs[neighbor] += self.transition['indifferent'] * beliefs[nowPosition]
	else:
	  # for most likely direction, put more weight on it
	  newBeliefs[neighbor] += self.transition['favor'] * beliefs[nowPosition]

    newBeliefs.normalize()

    # update the beliefs with new time slice
    beliefs = newBeliefs

  def findNearestPathToPacman(self, gameState, positions):
    pacmanPos = gameState.getAgentPosition(self.index)
    distanceFunc = lambda position : self.getMazeDistance(pacmanPos, position)
    return min(positions, distanceFunc)
    
  def findNearestPathToFood(self, gameState, positions):
    foodList = self.getFoodYouAreDefending(gameState).asList()
    minDis = 99999
    minPos = None
    for pos in positions:
      for food in foodList:
        dis = self.getMazeDistance(pos, food)
	if dis < minDis:
	  minDis = dis
	  minPos = pos

    return minPos

  def getSuccessor(self, gameState, action):
    """
    Finds the next successor which is a grid position (location tuple).
    """
    successor = gameState.generateSuccessor(self.index, action)
    pos = successor.getAgentState(self.index).getPosition()
    if pos != nearestPoint(pos):
      # Only half a grid position was covered
      return successor.generateSuccessor(self.index, action)
    else:
      return successor

  def evaluate(self, gameState, action):
    """
    Computes a linear combination of features and feature weights
    """
    features = self.getFeatures(gameState, action)
    weights = self.getWeights(gameState, action)
    return features * weights

  def getFeatures(self, gameState, action):
    """
    Returns a counter of features for the state
    """
    features = util.Counter()
    successor = self.getSuccessor(gameState, action)
    features['successorScore'] = self.getScore(successor)
    return features

  def getWeights(self, gameState, action):
    """
    Normally, weights do not depend on the gamestate.  They can be either
    a counter or a dictionary.
    """
    return {'successorScore': 1.0}

  def isOurTerrain(self, gameState, pos):
    """
    Check if pos is in our terrain
    """
    if self.red:
      return gameState.isRed(pos)
    else:
      return not gameState.isRed(pos)

  def setNeighbors(self, gameState):
    """
    this will set self.availableActions
    this is different from legalActions, which is for agents. this is for each grid and, thus, static
    """
    self.neighbors = util.Counter()
    for pos in self.legalPositions:
      self.neighbors[pos] = Actions.getLegalNeighbors(pos, gameState.data.layout.walls)

class OffensiveReflexAgent(ReflexCaptureAgent):
  """
  A reflex agent that seeks food. This is an agent
  we give you to get an idea of what an offensive agent might look like,
  but it is by no means the best or only way to build an offensive agent.
  """

  def getFeatures(self, gameState, action):
    global beliefs

    features = util.Counter()
    successor = self.getSuccessor(gameState, action)
    enemies = [successor.getAgentState(i) for i in self.getOpponents(successor)]
    ghosts = [a for a in enemies if not a.isPacman and a.getPosition() != None]
    features['successorScore'] = self.getScore(successor)
    
    # Compute distance to the nearest food
    foodList = self.getFood(successor).asList()
    if len(foodList) > 0: # This should always be True,  but better safe than sorry
      myPos = successor.getAgentState(self.index).getPosition()
      minDistance = min([self.getMazeDistance(myPos, food) for food in foodList])
      features['distanceToFood'] = minDistance

    # distance to the most likely ghost position
    if self.isOurTerrain(gameState, myPos):
      features['danger'] = 0
    else:
      ghostPosition = max([pos for pos in beliefs.keys() if pos in self.offensiveLegalPositions], key = lambda pos: beliefs[pos])

      # maybe our observation is scared. In that case, noWorry == True
      # I tried to implement this in belief, but failed - defensive agent does care about where the ghost is.
      # It would become pacman immediately after invading even scared
      noWorry = False 
      for ghost in ghosts:
        if ghost.scaredTimer > 0 and ghost.getPosition() == ghostPosition:
	  noWorry = True # okay, what we find is just a scared ghost
      likelyDistance = self.getMazeDistance(myPos, ghostPosition)
      if likelyDistance < 2 and not noWorry:
	features['danger'] = 2 - likelyDistance
      else:
        features['danger'] = 0

    return features

  def getWeights(self, gameState, action):
    return {'successorScore': 100, 'distanceToFood': -1, 'danger': -500}

class DefensiveReflexAgent(ReflexCaptureAgent):
  """
  A reflex agent that keeps its side Pacman-free. Again,
  this is to give you an idea of what a defensive agent
  could be like.  It is not the best or only way to make
  such an agent.
  """

  def getFeatures(self, gameState, action):
    features = util.Counter()
    successor = self.getSuccessor(gameState, action)

    myState = successor.getAgentState(self.index)
    myPos = myState.getPosition()

    # Computes whether we're on defense (1) or offense (0), encourage it to defense
    features['onDefense'] = 1
    if myState.isPacman: features['onDefense'] = 0

    # distance to pacman
    pacmanPosition = max([pos for pos in beliefs.keys() if pos in self.defensiveLegalPositions], key = lambda pos: beliefs[pos])
    print myPos, pacmanPosition
    likelyDistance = self.getMazeDistance(myPos, pacmanPosition)
    features['invaderDistance'] = likelyDistance

    # Computes number of invaders
    enemies = [successor.getAgentState(i) for i in self.getOpponents(successor)]
    invaders = [a for a in enemies if a.isPacman and a.getPosition() != None]
    features['numInvaders'] = len(invaders)

    if action == Directions.STOP: features['stop'] = 1
    rev = Directions.REVERSE[gameState.getAgentState(self.index).configuration.direction]
    if action == rev: features['reverse'] = 1

    return features

  def getWeights(self, gameState, action):
    return {'numInvaders': -1000, 'onDefense': 100, 'invaderDistance': -10, 'stop': -100, 'reverse': -2}


